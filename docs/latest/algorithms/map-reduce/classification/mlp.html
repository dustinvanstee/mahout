

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>(Deprecated)  Multilayer Perceptron</title>
  
  <meta name="author" content="Apache Mahout">

  <!-- Enable responsive viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Bootstrap styles -->
  <link href="/assets/themes/mahout3/css/bootstrap.min.css" rel="stylesheet">
  <!-- Optional theme -->
  <link href="/assets/themes/mahout3/css/bootstrap-theme.min.css" rel="stylesheet">
  <!-- Sticky Footer -->
  <link href="/assets/themes/mahout3/css/bs-sticky-footer.css" rel="stylesheet">

  <!-- Custom styles -->
  <link href="/assets/themes/mahout3/css/style.css" rel="stylesheet" type="text/css" media="all">

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->

  <!-- atom & rss feed -->
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
  </script>
  <script type="text/javascript">
    var mathjax = document.createElement('script');
    mathjax.type = 'text/javascript';
    mathjax.async = true;

    mathjax.src = ('https:' == document.location.protocol) ?
        'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' :
        'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';

      var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(mathjax, s);
  </script>
</head>

<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">
        <img src="/assets/img/Mahout-logo-82x100.png" height="30" alt="I'm mahout">
      </a>
    </div>

    


<!-- Collect the nav links, forms, and other content for toggling -->
<div class="collapse navbar-collapse" id="main-navbar">
    <ul class="nav navbar-nav">

        <!-- Quick Start -->
        <li id="quickstart">
            <a href="/index.html" >Mahout Overview</a>
        </li>

        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Key Concepts<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="/index.html">Mahout Overview</a></li>
                <li><span><b>&nbsp;&nbsp;Scala DSL</b><span></li>
                <li><a href="/mahout-samsara/in-core-reference.html">In-core Reference</a></li>
                <li><a href="/mahout-samsara/out-of-core-reference.html">Out-of-core Reference</a></li>
                <li><a href="/mahout-samsara/faq.html">Samsara FAQ</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Bindings</b><span></li>
                <li><a href="/distributed/spark-bindings/">Spark Bindings</a></li>
                <li><a href="/distributed/flink-bindings.html">Flink Bindings</a></li>
                <li><a href="/distributed/flink-bindings.html">H20 Bindings</a></li>
                <!--<li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Native Solvers</b><span></li>
                <li><a href="/native-solvers/viennacl.html">ViennaCL</a></li>
                <li><a href="/native-solvers/viennacl-omp.html">ViennaCL-OMP</a></li>
                <li><a href="/native-solvers/cuda.html">CUDA</a></li>-->
            </ul>
        </li>

        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Tutorials<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><span>&nbsp;&nbsp;<b>Reccomenders</b><span></li>
                <li><a href="/tutorials/cco-lastfm">CCO Example with Last.FM Data</a></li>
                <li><a href="/tutorials/intro-cooccurrence-spark">Introduction to Cooccurrence in Spark</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Mahout Samsara</b><span></li>
                <li><a href="/tutorials/samsara/play-with-shell.html">Playing with Samsara in Spark Shell</a></li>
                <li><a href="/tutorials/samsara/playing-with-samsara-flink-batch.html">Playing with Samsara in Flink Batch</a></li>
                <li><a href="/tutorials/samsara/classify-a-doc-from-the-shell.html">Text Classification (Shell)</a></li>
                <li><a href="/tutorials/samsara/spark-naive-bayes.html">Spark Naive Bayes</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Misc</b><span></li>
                <li><a href="/tutorials/misc/mahout-in-zeppelin">Mahout in Apache Zeppelin</a></li>
                <li><a href="/tutorials/misc/contributing-algos">How To Contribute a New Algorithm</a></li>
                <li><a href="/tutorials/misc/how-to-build-an-app.html">How To Build An App</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Deprecated</b><span></li>
                <li><a href="/tutorials/map-reduce">MapReduce</a></li>
            </ul>
        </li>


        <!-- Algorithms (Samsara / MR) -->
        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Algorithms<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="/algorithms/linear-algebra">Distributed Linear Algebra</a></li>
                <li><a href="/algorithms/preprocessors">Preprocessors</a></li>
                <li><a href="/algorithms/regression">Regression</a></li>
                <li><a href="/algorithms/reccomenders">Reccomenders</a></li>
                <li role="separator" class="divider"></li>
                <li><a href="/algorithms/map-reduce">MapReduce <i>(deprecated)</i></a></li>
            </ul>
                <!--<li><a href="/algorithms/reccomenders/recommender-overview.html">Reccomender Overview</a></li> Do we still need? seems like short version of next post-->
                <!--
                <li><a href="/algorithms/reccomenders/intro-cooccurrence-spark.html">Intro to Coocurrence With Spark</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<a href="/algorithms/map-reduce"><b>MapReduce</b> (deprecated)</a><span></li>


             -->
        </li>

        <!-- Scala Docs -->
        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">API Docs<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="/0.13.0/api/index.html">0.13.0</a></li>
            </ul>
        </li>


    </ul>
    <form class="navbar-form navbar-left">
        <div class="form-group">
            <input type="text" class="form-control" placeholder="Search">
        </div>
        <button type="submit" class="btn btn-default">Submit</button>
    </form>
    <ul class="nav navbar-nav navbar-right">
        <li><a href="http://github.com/apache/mahout">Github</a></li>

        <!-- Apache -->
        <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Apache <span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="http://www.apache.org/foundation/how-it-works.html">Apache Software Foundation</a></li>
                <li><a href="http://www.apache.org/licenses/">Apache License</a></li>
                <li><a href="http://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
                <li><a href="http://www.apache.org/foundation/thanks.html">Thanks</a></li>
            </ul>
        </li>

    </ul>
</div><!-- /.navbar-collapse -->

  </div><!-- /.container-fluid -->
</nav>

<body>

<div id="wrap">
  <body class="">

  <div class="container">
    


<div class="row">
    <div class="col-md-3">
        <div id="AlgoMenu">
    <span><b>Mahout-Samsara Algorithms</b></span>
    <div class="list-group panel">
        <a href="#linalg" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Linear Algebra</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="linalg">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/linear-algebra/d-qr.html">Distributed QR Decomposition</a></li>
                <li> <a href="/algorithms/linear-algebra/d-spca.html">Distributed Stochastic Principal Component Analysis</a></li>
                <li> <a href="/algorithms/linear-algebra/d-ssvd.html">Distributed Stochastic Singular Value Decomposition</a></li>
            </ul>
        </div>
        <a href="#clustering" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Clustering</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="clustering">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/clustering">Clustering Algorithms</a></li>
                <li> <a href="/algorithms/clustering/distance-metrics.html">Distance Metrics</a></li>
                <li> <a href="/algorithms/clustering/canopy">Canopy Clustering</a></li>
            </ul>
        </div>
        <a href="#preprocessors" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Preprocessors</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="preprocessors">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/preprocessors/AsFactor.html">AsFactor (a.k.a. One-Hot-Encoding)</a></li>
                <li> <a href="/algorithms/preprocessors/StandardScaler.html">StandardScaler</a></li>
                <li> <a href="/algorithms/preprocessors/MeanCenter.html">MeanCenter</a></li>
            </ul>
        </div>
        <a href="#regression" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Regression</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="regression">
            <ul class="nav sidebar-nav">
                <a href="#serial-correlation" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#regression"><b>&#8226;&nbsp;Serial Correlation</b><i class="fa fa-caret-down"></i></a>
                <div class="collapse" id="serial-correlation">
                    <ul class="nav sidebar-nav">
                        <li> <a href="/algorithms/regression/serial-correlation/cochrane-orcutt.html">Cochrane-Orcutt Procedure</a></li>
                        <li> <a href="/algorithms/regression/serial-correlation/dw-test.html">Durbin Watson Test</a></li>
                    </ul>
                </div>
                <li> <a href="/algorithms/regression/ols.html">Ordinary Least Squares (Closed Form)</a></li>
                <li> <a href="/algorithms/regression/fittness-tests.html">Fitness Tests</a></li>
            </ul>
        </div>
        <a href="#reccomenders" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Reccomenders</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="reccomenders">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/reccomenders">Reccomender Overview</a></li>
                <li> <a href="/algorithms/reccomenders/cco.html">CCO</a></li>
                <li> <a href="/algorithms/reccomenders/d-als.html">Distributed Alternating Least Squares</a></li>
            </ul>
        </div>
    </div>
    <span><b>Map Reduce Algorithms</b> (deprecated)</span>
    <div class="list-group panel">
        <a href="#classification" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Classification</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="classification">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/map-reduce/classification/bayesian.html">Bayesian</a></li>
                <li> <a href="/algorithms/map-reduce/classification/class-discovery.html">Class Discovery</a></li>
                <li> <a href="/algorithms/map-reduce/classification/classifyingyourdata.html">Classifying Your Data</a></li>
                <li> <a href="/algorithms/map-reduce/classification/collocations.html">Collocation</a></li>
                <li> <a href="/algorithms/map-reduce/classification/gaussian-discriminative-analysis.html">Gaussian Discriminative Analysis</a></li>
                <li> <a href="/algorithms/map-reduce/classification/hidden-markov-models.html">Hidden Markov Models</a></li>
                <li> <a href="/algorithms/map-reduce/classification/independent-component-analysis.html">Independent Component Analysis</a></li>
                <li> <a href="/algorithms/map-reduce/classification/locally-weighted-linear-regression.html">Locally Weighted Linear Regression</a></li>
                <li> <a href="/algorithms/map-reduce/classification/logistic-regression.html">Logistic Regression</a></li>
                <li> <a href="/algorithms/map-reduce/classification/mahout-collections.html">Mahout Collections</a></li>
                <li> <a href="/algorithms/map-reduce/classification/mlp.html">Multilayer Perceptron</a></li>
                <li> <a href="/algorithms/map-reduce/classification/naivebayes.html">Naive Bayes</a></li>
                <li> <a href="/algorithms/map-reduce/classification/neural-network.html">Neural Networks</a></li>
                <li> <a href="/algorithms/map-reduce/classification/partial-implementation.html">Partial Implementation</a></li>
                <li> <a href="/algorithms/map-reduce/classification/random-forrests.html">Random Forrests</a></li>
                <li> <a href="/algorithms/map-reduce/classification/restricted-boltzman-machines.html">Restricted Boltzman Machines</a></li>
                <li> <a href="/algorithms/map-reduce/classification/support-vector-machines.html">Support Vector Machines</a></li>
            </ul>
        </div>
        <a href="#mr-clustering" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Clustering</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="mr-clustering">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/map-reduce/clustering/canopy-clustering.html">Canopy Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/cluster-dumper.html">Cluster Dumper</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/expectation-maximization.html">Expectation Maximization</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/fuzzy-k-means.html">Fuzzy K-Means</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/hierarchical-clustering.html">Hierarchical Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/k-means-clustering.html">K-Means Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/latent-dirichlet-allocation.html">Latent Dirichlet Allocation</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/llr---log-likelihood-ratio.html">Log Likelihood Ratio</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/spectral-clustering.html">Spectral Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/streaming-k-means.html">Streaming K-Means</a></li>
            </ul>
        </div>
    </div>
</div>

    </div>

    <div class="col-md-8">
        <div class="page-header">
            <h1>(Deprecated)  Multilayer Perceptron </h1>
        </div>
        <h1 id="multilayer-perceptron">Multilayer Perceptron</h1>

<p>A multilayer perceptron is a biologically inspired feed-forward network that can 
be trained to represent a nonlinear mapping between input and output data. It 
consists of multiple layers, each containing multiple artificial neuron units and
can be used for classification and regression tasks in a supervised learning approach.</p>

<h2 id="command-line-usage">Command line usage</h2>

<p>The MLP implementation is currently located in the MapReduce-Legacy package. It
can be used with the following commands:</p>

<h1 id="model-training">model training</h1>
<pre><code>$ bin/mahout org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron  # model usage
$ bin/mahout org.apache.mahout.classifier.mlp.RunMultilayerPerceptron
</code></pre>

<p>To train and use the model, a number of parameters can be specified. Parameters without default values have to be specified by the user. Consider that not all parameters can be used both for training and running the model. We give an example of the usage below.</p>

<h3 id="parameters">Parameters</h3>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Command</th>
      <th style="text-align: right">Default</th>
      <th style="text-align: left">Description</th>
      <th style="text-align: left">Type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">–input -i</td>
      <td style="text-align: right"> </td>
      <td style="text-align: left">Path to the input data (currently, only .csv-files are allowed)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">–skipHeader -sh</td>
      <td style="text-align: right">false</td>
      <td style="text-align: left">Skip first row of the input file (corresponds to the csv headers)</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">–update -u</td>
      <td style="text-align: right">false</td>
      <td style="text-align: left">Whether the model should be updated incrementally with every new training instance. If this parameter is not given, the model is trained from scratch.</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–labels -labels</td>
      <td style="text-align: right"> </td>
      <td style="text-align: left">Instance labels separated by whitespaces.</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–model -mo</td>
      <td style="text-align: right"> </td>
      <td style="text-align: left">Location where the model will be stored / is stored (if the specified location has an existing model, it will update the model through incremental learning).</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">–layerSize -ls</td>
      <td style="text-align: right"> </td>
      <td style="text-align: left">Number of units per layer, including input, hidden and ouput layers. This parameter specifies the topology of the network (see <a href="mlperceptron_structure.png" title="Architecture of a three-layer MLP">this image</a> for an example specified by <code>-ls 4 8 3</code>).</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–squashingFunction -sf</td>
      <td style="text-align: right">Sigmoid</td>
      <td style="text-align: left">The squashing function to use for the units. Currently only the sigmoid fucntion is available.</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–learningRate -l</td>
      <td style="text-align: right">0.5</td>
      <td style="text-align: left">The learning rate that is used for weight updates.</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–momemtumWeight -m</td>
      <td style="text-align: right">0.1</td>
      <td style="text-align: left">The momentum weight that is used for gradient descent. Must be in the range between 0 … 1.0</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–regularizationWeight -r</td>
      <td style="text-align: right">0</td>
      <td style="text-align: left">Regularization value for the weight vector. Must be in the range between 0 … 0.1</td>
      <td style="text-align: left">training</td>
    </tr>
    <tr>
      <td style="text-align: left">–format -f</td>
      <td style="text-align: right">csv</td>
      <td style="text-align: left">Input file format. Currently only csv is supported.</td>
      <td style="text-align: left"> </td>
    </tr>
    <tr>
      <td style="text-align: left">–columnRange -cr</td>
      <td style="text-align: right"> </td>
      <td style="text-align: left">Range of the columns to use from the input file, starting with 0 (i.e. <code>-cr 0 5</code> for including the first six columns only)</td>
      <td style="text-align: left">testing</td>
    </tr>
    <tr>
      <td style="text-align: left">–output -o</td>
      <td style="text-align: right"> </td>
      <td style="text-align: left">Path to store the labeled results from running the model.</td>
      <td style="text-align: left">testing</td>
    </tr>
  </tbody>
</table>

<h2 id="example-usage">Example usage</h2>

<p>In this example, we will train a multilayer perceptron for classification on the iris data set. The iris flower data set contains data of three flower species where each datapoint consists of four features.
The dimensions of the data set are given through some flower parameters (sepal length, sepal width, …). All samples contain a label that indicates the flower species they belong to.</p>

<h3 id="training">Training</h3>

<p>To train our multilayer perceptron model from the command line, we call the following command</p>

<pre><code>$ bin/mahout org.apache.mahout.classifier.mlp.TrainMultilayerPerceptron \
            -i ./mrlegacy/src/test/resources/iris.csv -sh \
            -labels setosa versicolor virginica \
            -mo /tmp/model.model -ls 4 8 3 -l 0.2 -m 0.35 -r 0.0001
</code></pre>

<p>The individual parameters are explained in the following.</p>

<ul>
  <li><code>-i ./mrlegacy/src/test/resources/iris.csv</code> use the iris data set as input data</li>
  <li><code>-sh</code> since the file <code>iris.csv</code> contains a header row, this row needs to be skipped</li>
  <li><code>-labels setosa versicolor virginica</code> we specify, which class labels should be learnt (which are the flower species in this case)</li>
  <li><code>-mo /tmp/model.model</code> specify where to store the model file</li>
  <li><code>-ls 4 8 3</code> we specify the structure and depth of our layers. The actual network structure can be seen in the figure below.</li>
  <li><code>-l 0.2</code> we set the learning rate to <code>0.2</code></li>
  <li><code>-m 0.35</code> momemtum weight is set to <code>0.35</code></li>
  <li><code>-r 0.0001</code> regularization weight is set to <code>0.0001</code></li>
</ul>

<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>The picture shows the architecture defined by the above command. The topolgy of the network is completely defined through the number of layers and units because in this implementation of the MLP every unit is fully connected to the units of the next and previous layer. Bias units are added automatically.</td>
      <td><img src="mlperceptron_structure.png" alt="Multilayer perceptron network" title="Architecture of a three-layer MLP" /></td>
    </tr>
  </tbody>
</table>

<h3 id="testing">Testing</h3>

<p>To test / run the multilayer perceptron classification on the trained model, we can use the following command</p>

<pre><code>$ bin/mahout org.apache.mahout.classifier.mlp.RunMultilayerPerceptron \
            -i ./mrlegacy/src/test/resources/iris.csv -sh -cr 0 3 \
            -mo /tmp/model.model -o /tmp/labelResult.txt
</code></pre>

<p>The individual parameters are explained in the following.</p>

<ul>
  <li><code>-i ./mrlegacy/src/test/resources/iris.csv</code> use the iris data set as input data</li>
  <li><code>-sh</code> since the file <code>iris.csv</code> contains a header row, this row needs to be skipped</li>
  <li><code>-cr 0 3</code> we specify the column range of the input file</li>
  <li><code>-mo /tmp/model.model</code> specify where the model file is stored</li>
  <li><code>-o /tmp/labelResult.txt</code> specify where the labeled output file will be stored</li>
</ul>

<h2 id="implementation">Implementation</h2>

<p>The Multilayer Perceptron implementation is based on a more general Neural Network class. Command line support was added later on and provides a simple usage of the MLP as shown in the example. It is implemented to run on a single machine using stochastic gradient descent where the weights are updated using one datapoint at a time, resulting in a weight update of the form:
<script type="math/tex">\vec{w}^{(t + 1)} = \vec{w}^{(t)} - n \Delta E_n(\vec{w}^{(t)})</script></p>

<p>where <em>a</em> is the activation of the unit. It is not yet possible to change the learning to more advanced methods using adaptive learning rates yet.</p>

<p>The number of layers and units per layer can be specified manually and determines the whole topology with each unit being fully connected to the previous layer. A bias unit is automatically added to the input of every layer. 
Currently, the logistic sigmoid is used as a squashing function in every hidden and output layer. It is of the form:</p>

<script type="math/tex; mode=display">\frac{1}{1 + exp(-a)}</script>

<p>The command line version <strong>does not perform iterations</strong> which leads to bad results on small datasets. Another restriction is, that the CLI version of the MLP only supports classification, since the labels have to be given explicitly when executing on the command line.</p>

<p>A learned model can be stored and updated with new training instanced using the <code>--update</code> flag. Output of classification reults is saved as a .txt-file and only consists of the assigned labels. Apart from the command-line interface, it is possible to construct and compile more specialized neural networks using the API and interfaces in the mrlegacy package.</p>

<h2 id="theoretical-background">Theoretical Background</h2>

<p>The <em>multilayer perceptron</em> was inspired by the biological structure of the brain where multiple neurons are connected and form columns and layers. Perceptual input enters this network through our sensory organs and is then further processed into higher levels. 
The term multilayer perceptron is a little misleading since the <em>perceptron</em> is a special case of a single <em>artificial neuron</em> that can be used for simple computations <a href="http://en.wikipedia.org/wiki/Perceptron" title="The perceptron in wikipedia">[1]</a>. The difference is that the perceptron uses a discontinous nonlinearity while for the MLP neurons that are implemented in mahout it is important to use continous nonlinearities. This is necessary for the implemented learning algorithm, where the error is propagated back from the output layer to the input layer and the weights of the connections are changed according to their contribution to the overall error. This algorithm is called backpropagation and uses gradient descent to update the weights. To compute the gradients we need continous nonlinearities. But let’s start from the beginning!</p>

<p>The first layer of the MLP represents the input and has no other purpose than routing the input to every connected unit in a feed-forward fashion. Following layers are called hidden layers and the last layer serves the special purpose to determine the output. The activation of a unit <em>u</em> in a hidden layer is computed through a weighted sum of all inputs, resulting in 
<script type="math/tex">a_j = \sum_{i=1}^{D} w_{ji}^{(l)} x_i + w_{j0}^{(l)}</script>
This computes the activation <em>a</em> for neuron <em>j</em> where <em>w</em> is the weight from neuron <em>i</em> to neuron <em>j</em> in layer <em>l</em>. The last part, where <em>i = 0</em> is called the bias and can be used as an offset, independent from the input.</p>

<p>The activation is then transformed by the aforementioned differentiable, nonlinear <em>activation function</em> and serves as the input to the next layer. The activation function is usually chosen from the family of sigmoidal functions such as <em>tanh</em> or <em>logistic sigmoidal</em> <a href="http://en.wikipedia.org/wiki/Sigmoid_function" title="Sigmoid function on wikipedia">[2]</a>. Often sigmoidal and logistic sigmoidal are used synonymous. Another word for the activation function is <em>squashing function</em> since the s-shape of this function class <em>squashes</em> the input.</p>

<p>For different units or layers, different activation functions can be used to obtain different behaviors. Especially in the output layer, the activation function can be chosen to obtain the output value <em>y</em>, depending on the learning problem:
<script type="math/tex">y_k = \sigma (a_k)</script></p>

<p>If the learning problem is a linear regression task, sigma can be chosen to be the identity function. In case of classification problems, the choice of the squashing functions depends on the exact task at hand and often softmax activation functions are used.</p>

<p>The equation for a MLP with three layers (one input, one hidden and one output) is then given by</p>

<script type="math/tex; mode=display">y_k(\vec{x}, \vec{w}) = h \left( \sum_{j=1}^{M} w_{kj}^{(2)} h \left( \sum_{i=1}^{D} w_{ji}^{(1)} x_i + w_{j0}^{(1)} \right) + w_{k0}^{(2)} \right)</script>

<p>where <em>h</em> indicates the respective squashing function that is used in the units of a layer. <em>M</em> and <em>D</em> specify the number of incoming connections to a unit and we can see that the input to the first layer (hidden layer) is just the original input <em>x</em> whereas the input into the second layer (output layer) is the transformed output of layer one. The output <em>y</em> of unit <em>k</em> is therefore given by the above equation and depends on the input <em>x</em> and the weight vector <em>w</em>. This shows us, that the parameter that we can optimize during learning is <em>w</em> since we can not do anything about the input <em>x</em>. To facilitate the following steps, we can include the bias-terms into the weight vector and correct for the indices by adding another dimension with the value 1 to the input vector. The bias is a constant factor that is added to the weighted sum and that serves as a scaling factor of the nonlinear transformation. Including it into the weight vector leads to:</p>

<script type="math/tex; mode=display">y_k(\vec{x}, \vec{w}) = h \left( \sum_{j=0}^{M} w_{kj}^{(2)} h \left( \sum_{i=0}^{D} w_{ji}^{(1)} x_i \right) \right)</script>

<p>The previous paragraphs described how the MLP transforms a given input into some output using a combination of different nonlinear functions. Of course what we really want is to learn the structure of our data so that we can feed data with unknown labels into the network and get the estimated target labels <em>t</em>. To achieve this, we have to train our network. In this context, training means optimizing some function such that the error between the real labels <em>y</em> and the network-output <em>t</em> becomes smallest. We have seen in the previous pragraph, that our only knob to change is the weight vector <em>w</em>, making the function to be optimized a function of <em>w</em>. For simplicitly and because it is widely used, we choose the so called <em>sum-of-squares</em> error function as an example that is given by</p>

<script type="math/tex; mode=display">E(\vec{w}) = \frac{1}{2} \sum_{n=1}^N \left( y(\vec{x}_n, \vec{w}) - t_n \right)^2</script>

<p>The goal is to minimize this function and thereby increase the performance of our model. A common method to achieve this is to use gradient descent and the so called technique of <em>backpropagation</em> where the goal is to compute the contribution of every unit to the overall error and changing the weight according to this contribution and into the direction of the gradient of the error function at this particular unit. In the following we try to give a short overview of the model training with gradient descent and backpropagation. A more detailed example can be found in <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/" title="Christopher M. Bishop: Pattern Recognition and Machine Learning, Springer 2009">[3]</a> where much of this information is taken from.</p>

<p>The problem with minimizing the error function is that the error can only be computed at the output layers where we get <em>t</em>, but we want to update all the weights of all the units. Therefore we use the technique of backpropagation to propagate the error, that we first compute at the output layer, back to the units of the previous layers. For this approach we also need to compute the gradients of the activation function.</p>

<p>Weights are then updated with a small step in the direction of the negative gradient, regulated by the learning rate <em>n</em> such that we arrive at the formula for weight update:</p>

<script type="math/tex; mode=display">\vec{w}^{(t + 1)} = \vec{w}^{(t)} - n \Delta E(\vec{w}^{(t)})</script>

<p>A momentum weight can be set as a parameter of the gradient descent method to increase the probability of finding better local or global optima of the error function.</p>

<p>References</p>

<p>[1] http://en.wikipedia.org/wiki/Perceptron</p>

<p>[2] http://en.wikipedia.org/wiki/Sigmoid_function</p>

<p>[3] <a href="http://research.microsoft.com/en-us/um/people/cmbishop/prml/">Christopher M. Bishop: Pattern Recognition and Machine Learning, Springer 2009</a></p>


    </div>
</div>

  </div>


</div>

<div id="footer">
  <div class="container">
    <p>&copy; 2017 Apache Mahout
      with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
      and <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>
    </p>
  </div>
</div>







<!-- Latest compiled and minified JavaScript, requires jQuery 1.x (2.x not supported in IE8) -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="/assets/themes/mahout3/js/bootstrap.min.js"></script>
</body>
</html>

