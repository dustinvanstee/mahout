

<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>(Deprecated)  Spectral Clustering</title>
  
  <meta name="author" content="Apache Mahout">

  <!-- Enable responsive viewport -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Bootstrap styles -->
  <link href="/assets/themes/mahout3/css/bootstrap.min.css" rel="stylesheet">
  <!-- Optional theme -->
  <link href="/assets/themes/mahout3/css/bootstrap-theme.min.css" rel="stylesheet">
  <!-- Sticky Footer -->
  <link href="/assets/themes/mahout3/css/bs-sticky-footer.css" rel="stylesheet">

  <!-- Custom styles -->
  <link href="/assets/themes/mahout3/css/style.css" rel="stylesheet" type="text/css" media="all">

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
  <![endif]-->

  <!-- Fav and touch icons -->
  <!-- Update these with your own images
    <link rel="shortcut icon" href="images/favicon.ico">
    <link rel="apple-touch-icon" href="images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="72x72" href="images/apple-touch-icon-72x72.png">
    <link rel="apple-touch-icon" sizes="114x114" href="images/apple-touch-icon-114x114.png">
  -->

  <!-- atom & rss feed -->
  <link href="/atom.xml" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
  <link href="/rss.xml" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
  </script>
  <script type="text/javascript">
    var mathjax = document.createElement('script');
    mathjax.type = 'text/javascript';
    mathjax.async = true;

    mathjax.src = ('https:' == document.location.protocol) ?
        'https://c328740.ssl.cf1.rackcdn.com/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML' :
        'http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';

      var s = document.getElementsByTagName('script')[0];
    s.parentNode.insertBefore(mathjax, s);
  </script>
</head>

<nav class="navbar navbar-default navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">
        <img src="/assets/img/Mahout-logo-82x100.png" height="30" alt="I'm mahout">
      </a>
    </div>

    


<!-- Collect the nav links, forms, and other content for toggling -->
<div class="collapse navbar-collapse" id="main-navbar">
    <ul class="nav navbar-nav">

        <!-- Quick Start -->
        <li id="quickstart">
            <a href="/index.html" >Mahout Overview</a>
        </li>

        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Key Concepts<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="/index.html">Mahout Overview</a></li>
                <li><span><b>&nbsp;&nbsp;Scala DSL</b><span></li>
                <li><a href="/mahout-samsara/in-core-reference.html">In-core Reference</a></li>
                <li><a href="/mahout-samsara/out-of-core-reference.html">Out-of-core Reference</a></li>
                <li><a href="/mahout-samsara/faq.html">Samsara FAQ</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Bindings</b><span></li>
                <li><a href="/distributed/spark-bindings/">Spark Bindings</a></li>
                <li><a href="/distributed/flink-bindings.html">Flink Bindings</a></li>
                <li><a href="/distributed/flink-bindings.html">H20 Bindings</a></li>
                <!--<li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Native Solvers</b><span></li>
                <li><a href="/native-solvers/viennacl.html">ViennaCL</a></li>
                <li><a href="/native-solvers/viennacl-omp.html">ViennaCL-OMP</a></li>
                <li><a href="/native-solvers/cuda.html">CUDA</a></li>-->
            </ul>
        </li>

        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Tutorials<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><span>&nbsp;&nbsp;<b>Reccomenders</b><span></li>
                <li><a href="/tutorials/cco-lastfm">CCO Example with Last.FM Data</a></li>
                <li><a href="/tutorials/intro-cooccurrence-spark">Introduction to Cooccurrence in Spark</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Mahout Samsara</b><span></li>
                <li><a href="/tutorials/samsara/play-with-shell.html">Playing with Samsara in Spark Shell</a></li>
                <li><a href="/tutorials/samsara/playing-with-samsara-flink-batch.html">Playing with Samsara in Flink Batch</a></li>
                <li><a href="/tutorials/samsara/classify-a-doc-from-the-shell.html">Text Classification (Shell)</a></li>
                <li><a href="/tutorials/samsara/spark-naive-bayes.html">Spark Naive Bayes</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Misc</b><span></li>
                <li><a href="/tutorials/misc/mahout-in-zeppelin">Mahout in Apache Zeppelin</a></li>
                <li><a href="/tutorials/misc/contributing-algos">How To Contribute a New Algorithm</a></li>
                <li><a href="/tutorials/misc/how-to-build-an-app.html">How To Build An App</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<b>Deprecated</b><span></li>
                <li><a href="/tutorials/map-reduce">MapReduce</a></li>
            </ul>
        </li>


        <!-- Algorithms (Samsara / MR) -->
        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Algorithms<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="/algorithms/linear-algebra">Distributed Linear Algebra</a></li>
                <li><a href="/algorithms/preprocessors">Preprocessors</a></li>
                <li><a href="/algorithms/regression">Regression</a></li>
                <li><a href="/algorithms/reccomenders">Reccomenders</a></li>
                <li role="separator" class="divider"></li>
                <li><a href="/algorithms/map-reduce">MapReduce <i>(deprecated)</i></a></li>
            </ul>
                <!--<li><a href="/algorithms/reccomenders/recommender-overview.html">Reccomender Overview</a></li> Do we still need? seems like short version of next post-->
                <!--
                <li><a href="/algorithms/reccomenders/intro-cooccurrence-spark.html">Intro to Coocurrence With Spark</a></li>
                <li role="separator" class="divider"></li>
                <li><span>&nbsp;&nbsp;<a href="/algorithms/map-reduce"><b>MapReduce</b> (deprecated)</a><span></li>


             -->
        </li>

        <!-- Scala Docs -->
        <li id="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">API Docs<span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="/0.13.0/api/index.html">0.13.0</a></li>
            </ul>
        </li>


    </ul>
    <form class="navbar-form navbar-left">
        <div class="form-group">
            <input type="text" class="form-control" placeholder="Search">
        </div>
        <button type="submit" class="btn btn-default">Submit</button>
    </form>
    <ul class="nav navbar-nav navbar-right">
        <li><a href="http://github.com/apache/mahout">Github</a></li>

        <!-- Apache -->
        <li class="dropdown">
            <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Apache <span class="caret"></span></a>
            <ul class="dropdown-menu">
                <li><a href="http://www.apache.org/foundation/how-it-works.html">Apache Software Foundation</a></li>
                <li><a href="http://www.apache.org/licenses/">Apache License</a></li>
                <li><a href="http://www.apache.org/foundation/sponsorship.html">Sponsorship</a></li>
                <li><a href="http://www.apache.org/foundation/thanks.html">Thanks</a></li>
            </ul>
        </li>

    </ul>
</div><!-- /.navbar-collapse -->

  </div><!-- /.container-fluid -->
</nav>

<body>

<div id="wrap">
  <body class="">

  <div class="container">
    


<div class="row">
    <div class="col-md-3">
        <div id="AlgoMenu">
    <span><b>Mahout-Samsara Algorithms</b></span>
    <div class="list-group panel">
        <a href="#linalg" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Linear Algebra</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="linalg">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/linear-algebra/d-qr.html">Distributed QR Decomposition</a></li>
                <li> <a href="/algorithms/linear-algebra/d-spca.html">Distributed Stochastic Principal Component Analysis</a></li>
                <li> <a href="/algorithms/linear-algebra/d-ssvd.html">Distributed Stochastic Singular Value Decomposition</a></li>
            </ul>
        </div>
        <a href="#clustering" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Clustering</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="clustering">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/clustering">Clustering Algorithms</a></li>
                <li> <a href="/algorithms/clustering/distance-metrics.html">Distance Metrics</a></li>
                <li> <a href="/algorithms/clustering/canopy">Canopy Clustering</a></li>
            </ul>
        </div>
        <a href="#preprocessors" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Preprocessors</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="preprocessors">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/preprocessors/AsFactor.html">AsFactor (a.k.a. One-Hot-Encoding)</a></li>
                <li> <a href="/algorithms/preprocessors/StandardScaler.html">StandardScaler</a></li>
                <li> <a href="/algorithms/preprocessors/MeanCenter.html">MeanCenter</a></li>
            </ul>
        </div>
        <a href="#regression" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Regression</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="regression">
            <ul class="nav sidebar-nav">
                <a href="#serial-correlation" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#regression"><b>&#8226;&nbsp;Serial Correlation</b><i class="fa fa-caret-down"></i></a>
                <div class="collapse" id="serial-correlation">
                    <ul class="nav sidebar-nav">
                        <li> <a href="/algorithms/regression/serial-correlation/cochrane-orcutt.html">Cochrane-Orcutt Procedure</a></li>
                        <li> <a href="/algorithms/regression/serial-correlation/dw-test.html">Durbin Watson Test</a></li>
                    </ul>
                </div>
                <li> <a href="/algorithms/regression/ols.html">Ordinary Least Squares (Closed Form)</a></li>
                <li> <a href="/algorithms/regression/fittness-tests.html">Fitness Tests</a></li>
            </ul>
        </div>
        <a href="#reccomenders" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Reccomenders</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="reccomenders">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/reccomenders">Reccomender Overview</a></li>
                <li> <a href="/algorithms/reccomenders/cco.html">CCO</a></li>
                <li> <a href="/algorithms/reccomenders/d-als.html">Distributed Alternating Least Squares</a></li>
            </ul>
        </div>
    </div>
    <span><b>Map Reduce Algorithms</b> (deprecated)</span>
    <div class="list-group panel">
        <a href="#classification" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Classification</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="classification">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/map-reduce/classification/bayesian.html">Bayesian</a></li>
                <li> <a href="/algorithms/map-reduce/classification/class-discovery.html">Class Discovery</a></li>
                <li> <a href="/algorithms/map-reduce/classification/classifyingyourdata.html">Classifying Your Data</a></li>
                <li> <a href="/algorithms/map-reduce/classification/collocations.html">Collocation</a></li>
                <li> <a href="/algorithms/map-reduce/classification/gaussian-discriminative-analysis.html">Gaussian Discriminative Analysis</a></li>
                <li> <a href="/algorithms/map-reduce/classification/hidden-markov-models.html">Hidden Markov Models</a></li>
                <li> <a href="/algorithms/map-reduce/classification/independent-component-analysis.html">Independent Component Analysis</a></li>
                <li> <a href="/algorithms/map-reduce/classification/locally-weighted-linear-regression.html">Locally Weighted Linear Regression</a></li>
                <li> <a href="/algorithms/map-reduce/classification/logistic-regression.html">Logistic Regression</a></li>
                <li> <a href="/algorithms/map-reduce/classification/mahout-collections.html">Mahout Collections</a></li>
                <li> <a href="/algorithms/map-reduce/classification/mlp.html">Multilayer Perceptron</a></li>
                <li> <a href="/algorithms/map-reduce/classification/naivebayes.html">Naive Bayes</a></li>
                <li> <a href="/algorithms/map-reduce/classification/neural-network.html">Neural Networks</a></li>
                <li> <a href="/algorithms/map-reduce/classification/partial-implementation.html">Partial Implementation</a></li>
                <li> <a href="/algorithms/map-reduce/classification/random-forrests.html">Random Forrests</a></li>
                <li> <a href="/algorithms/map-reduce/classification/restricted-boltzman-machines.html">Restricted Boltzman Machines</a></li>
                <li> <a href="/algorithms/map-reduce/classification/support-vector-machines.html">Support Vector Machines</a></li>
            </ul>
        </div>
        <a href="#mr-clustering" class="list-group-item list-group-item-success" data-toggle="collapse" data-parent="#AlgoMenu"><b>Clustering</b><i class="fa fa-caret-down"></i></a>
        <div class="collapse" id="mr-clustering">
            <ul class="nav sidebar-nav">
                <li> <a href="/algorithms/map-reduce/clustering/canopy-clustering.html">Canopy Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/cluster-dumper.html">Cluster Dumper</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/expectation-maximization.html">Expectation Maximization</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/fuzzy-k-means.html">Fuzzy K-Means</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/hierarchical-clustering.html">Hierarchical Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/k-means-clustering.html">K-Means Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/latent-dirichlet-allocation.html">Latent Dirichlet Allocation</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/llr---log-likelihood-ratio.html">Log Likelihood Ratio</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/spectral-clustering.html">Spectral Clustering</a></li>
                <li> <a href="/algorithms/map-reduce/clustering/streaming-k-means.html">Streaming K-Means</a></li>
            </ul>
        </div>
    </div>
</div>

    </div>

    <div class="col-md-8">
        <div class="page-header">
            <h1>(Deprecated)  Spectral Clustering </h1>
        </div>
        <h1 id="streamingkmeans-algorithm"><em>StreamingKMeans</em> algorithm</h1>

<p>The <em>StreamingKMeans</em> algorithm is a variant of Algorithm 1 from <a href="http://nips.cc/Conferences/2011/Program/event.php?ID=2989" title="M. Shindler, A. Wong, A. Meyerson: Fast and Accurate k-means For Large Datasets">Shindler et al</a> and consists of two steps:</p>

<ol>
  <li>Streaming step</li>
  <li>BallKMeans step.</li>
</ol>

<p>The streaming step is a randomized algorithm that makes one pass through the data and 
produces as many centroids as it determines is optimal. This step can be viewed as 
a preparatory dimensionality reduction. If the size of the data stream is <em>n</em> and the 
expected number of clusters is <em>k</em>, the streaming step will produce roughly <em>k*log(n)</em> 
clusters that will be passed on to the BallKMeans step which will further reduce the 
number of clusters down to <em>k</em>. BallKMeans is a randomized Lloyd-type algorithm that
has been studied in detail, see <a href="http://www.math.uwaterloo.ca/~cswamy/papers/kmeansfnl.pdf" title="R. Ostrovsky, Y. Rabani, L. Schulman, Ch. Swamy: The Effectiveness of Lloyd-Type Methods for the k-means Problem">Ostrovsky et al</a>.</p>

<h2 id="streaming-step">Streaming step</h2>

<hr />

<h3 id="overview">Overview</h3>

<p>The streaming step is a derivative of the streaming 
portion of Algorithm 1 in <a href="http://nips.cc/Conferences/2011/Program/event.php?ID=2989" title="M. Shindler, A. Wong, A. Meyerson: Fast and Accurate k-means For Large Datasets">Shindler et al</a>. The main difference between the two is that 
Algorithm 1 of <a href="http://nips.cc/Conferences/2011/Program/event.php?ID=2989" title="M. Shindler, A. Wong, A. Meyerson: Fast and Accurate k-means For Large Datasets">Shindler et al</a> assumes 
the knowledge of the size of the data stream and uses it to set a key parameter 
for the algorithm. More precisely, the initial <em>distanceCutoff</em> (defined below), which is 
denoted by <em>f</em> in <a href="http://nips.cc/Conferences/2011/Program/event.php?ID=2989" title="M. Shindler, A. Wong, A. Meyerson: Fast and Accurate k-means For Large Datasets">Shindler et al</a>, is set to <em>1/(k(1+log(n))</em>. The <em>distanceCutoff</em> influences the number of clusters that the algorithm 
will produce. 
In contrast, Mahout implementation does not require the knowledge of the size of the 
data stream. Instead, it dynamically re-evaluates the parameters that depend on the size 
of the data stream at runtime as more and more data is processed. In particular, 
the parameter <em>numClusters</em> (defined below) changes its value as the data is processed.</p>

<p>###Parameters</p>

<ul>
  <li><strong>numClusters</strong> (int): Conceptually, <em>numClusters</em> represents the algorithm’s guess at the optimal 
number of clusters it is shooting for. In particular, <em>numClusters</em> will increase at run 
time as more and more data is processed. Note that •numClusters• is not the number of clusters that the algorithm will produce. Also, <em>numClusters</em> should not be set to the final number of clusters that we expect to receive as the output of <em>StreamingKMeans</em>.</li>
  <li><strong>distanceCutoff</strong> (double): a parameter representing the value of the distance between a point and 
its closest centroid after which
the new point will definitely be assigned to a new cluster. <em>distanceCutoff</em> can be thought 
of as an estimate of the variable <em>f</em> from Shindler et al. The default initial value for 
<em>distanceCutoff</em> is <em>1.0/numClusters</em> and <em>distanceCutoff</em> grows as a geometric progression with 
common ratio <em>beta</em> (see below).</li>
  <li><strong>beta</strong> (double): a constant parameter that controls the growth of <em>distanceCutoff</em>. If the initial setting of <em>distanceCutoff</em> is <em>d0</em>, <em>distanceCutoff</em> will grow as the geometric progression with initial term <em>d0</em> and common ratio <em>beta</em>. The default value for <em>beta</em> is 1.3.</li>
  <li><strong>clusterLogFactor</strong> (double): a constant parameter such that <em>clusterLogFactor</em> <em>log(numProcessedPoints)</em> is the runtime estimate of the number of clusters to be produced by the streaming step. If the final number of clusters (that we expect <em>StreamingKMeans</em> to output) is <em>k</em>, <em>clusterLogFactor</em> can be set to <em>k</em>.</li>
  <li><strong>clusterOvershoot</strong> (double): a constant multiplicative slack factor that slows down the collapsing of clusters. The default value is 2.</li>
</ul>

<p>###Algorithm</p>

<p>The algorithm processes the data one-by-one and makes only one pass through the data.
The first point from the data stream will form the centroid of the first cluster (this designation may change as more points are processed). Suppose there are <em>r</em> clusters at one point and a new point <em>p</em> is being processed. The new point can either be added to one of the existing <em>r</em> clusters or become a new cluster. To decide:</p>

<ul>
  <li>let <em>c</em> be the closest cluster to point <em>p</em></li>
  <li>let <em>d</em> be the distance between <em>c</em> and <em>p</em></li>
  <li>if <em>d &gt; distanceCutoff</em>, create a new cluster from <em>p</em> (<em>p</em> is too far away from the clusters to be part of any one of them)</li>
  <li>else (<em>d &lt;= distanceCutoff</em>), create a new cluster with probability <em>d / distanceCutoff</em> (the probability of creating a new cluster increases as <em>d</em> increases).</li>
</ul>

<p>There will be either <em>r</em> or <em>r+1</em> clusters after processing a new point.</p>

<p>As the number of clusters increases, it will go over the  <em>clusterOvershoot * numClusters</em> limit (<em>numClusters</em> represents a recommendation for the number of clusters that the streaming step should aim for and <em>clusterOvershoot</em> is the slack). To decrease the number of clusters the existing clusters
are treated as data points and are re-clustered (collapsed). This tends to make the number of clusters go down. If the number of clusters is still too high, <em>distanceCutoff</em> is increased.</p>

<h2 id="ballkmeans-step">BallKMeans step</h2>
<hr />
<h3 id="overview-1">Overview</h3>
<p>The algorithm is a Lloyd-type algorithm that takes a set of weighted vectors and returns k centroids, see <a href="http://www.math.uwaterloo.ca/~cswamy/papers/kmeansfnl.pdf" title="R. Ostrovsky, Y. Rabani, L. Schulman, Ch. Swamy: The Effectiveness of Lloyd-Type Methods for the k-means Problem">Ostrovsky et al</a> for details. The algorithm has two stages:</p>

<ol>
  <li>Seeding</li>
  <li>Ball k-means</li>
</ol>

<p>The seeding stage is an initial guess of where the centroids should be. The initial guess is improved using the ball k-means stage.</p>

<h3 id="parameters">Parameters</h3>

<ul>
  <li>
    <p><strong>numClusters</strong> (int): the number k of centroids to return.  The algorithm will return exactly this number of centroids.</p>
  </li>
  <li>
    <p><strong>maxNumIterations</strong> (int): After seeding, the iterative clustering procedure will be run at most <em>maxNumIterations</em> times.  1 or 2 iterations are recommended.  Increasing beyond this will increase the accuracy of the result at the expense of runtime.  Each successive iteration yields diminishing returns in lowering the cost.</p>
  </li>
  <li>
    <p><strong>trimFraction</strong> (double): Outliers are ignored when computing the center of mass for a cluster.  For any datapoint <em>x</em>, let <em>c</em> be the nearest centroid.  Let <em>d</em> be the minimum distance from <em>c</em> to another centroid.  If the distance from <em>x</em> to <em>c</em> is greater than <em>trimFraction * d</em>, then <em>x</em> is considered an outlier during that iteration of ball k-means.  The default is 9/10.  In <a href="http://www.math.uwaterloo.ca/~cswamy/papers/kmeansfnl.pdf" title="R. Ostrovsky, Y. Rabani, L. Schulman, Ch. Swamy: The Effectiveness of Lloyd-Type Methods for the k-means Problem">Ostrovsky et al</a>, the authors use <em>trimFraction</em> = 1/3, but this does not mean that 1/3 is optimal in practice.</p>
  </li>
  <li>
    <p><strong>kMeansPlusPlusInit</strong> (boolean): If true, the seeding method is k-means++.  If false, the seeding method is to select points uniformly at random.  The default is true.</p>
  </li>
  <li>
    <p><strong>correctWeights</strong> (boolean): If <em>correctWeights</em> is true, outliers will be considered when calculating the weight of centroids.  The default is true. Note that outliers are not considered when calculating the position of centroids.</p>
  </li>
  <li>
    <p><strong>testProbability</strong> (double): If <em>testProbability</em> is <em>p</em> (0 &lt; <em>p</em> &lt; 1), the data (of size n) is partitioned into a test set (of size <em>p*n</em>) and a training set (of size <em>(1-p)*n</em>).  If 0, no test set is created (the entire data set is used for both training and testing).  The default is 0.1 if <em>numRuns</em> &gt; 1.  If <em>numRuns</em> = 1, then no test set should be created (since it is only used to compare the cost between different runs).</p>
  </li>
  <li>
    <p><strong>numRuns</strong> (int): This is the number of runs to perform. The solution of lowest cost is returned.  The default is 1 run.</p>
  </li>
</ul>

<p>###Algorithm
The algorithm can be instructed to take multiple independent runs (using the <em>numRuns</em> parameter) and the algorithm will select the best solution (i.e., the one with the lowest cost). In practice, one run is sufficient to find a good solution.</p>

<p>Each run operates as follows: a seeding procedure is used to select k centroids, and then ball k-means is run iteratively to refine the solution.</p>

<p>The seeding procedure can be set to either ‘uniformly at random’ or ‘k-means++’ using <em>kMeansPlusPlusInit</em> boolean variable. Seeding with k-means++ involves more computation but offers better results in practice.</p>

<p>Each iteration of ball k-means runs as follows:</p>

<ol>
  <li>Clusters are formed by assigning each datapoint to the nearest centroid</li>
  <li>The centers of mass of the trimmed clusters (see <em>trimFraction</em> parameter above) become the new centroids</li>
</ol>

<p>The data may be partitioned into a test set and a training set (see <em>testProbability</em>). The seeding procedure and ball k-means run on the training set. The cost is computed on the test set.</p>

<p>##Usage of <em>StreamingKMeans</em></p>

<pre><code> bin/mahout streamingkmeans  
   -i &lt;input&gt;  
   -o &lt;output&gt; 
   -ow  
   -k &lt;k&gt;  
   -km &lt;estimatedNumMapClusters&gt;  
   -e &lt;estimatedDistanceCutoff&gt;  
   -mi &lt;maxNumIterations&gt;  
   -tf &lt;trimFraction&gt;  
   -ri                  
   -iw  
   -testp &lt;testProbability&gt;  
   -nbkm &lt;numBallKMeansRuns&gt;  
   -dm &lt;distanceMeasure&gt;   
   -sc &lt;searcherClass&gt;  
   -np &lt;numProjections&gt;  
   -s &lt;searchSize&gt;   
   -rskm  
   -xm &lt;method&gt;  
   -h   
   --tempDir &lt;tempDir&gt;   
   --startPhase &lt;startPhase&gt;   
   --endPhase &lt;endPhase&gt;                    
</code></pre>

<p>###Details on Job-Specific Options:</p>

<ul>
  <li><code>--input (-i) &lt;input&gt;</code>: Path to job input directory.</li>
  <li><code>--output (-o) &lt;output&gt;</code>: The directory pathname for output.</li>
  <li><code>--overwrite (-ow)</code>: If present, overwrite the output directory before running job.</li>
  <li><code>--numClusters (-k) &lt;k&gt;</code>: The k in k-Means. Approximately this many clusters will be generated.</li>
  <li><code>--estimatedNumMapClusters (-km) &lt;estimatedNumMapClusters&gt;</code>: The estimated number of clusters to use for the Map phase of the job when running StreamingKMeans. This should be around k * log(n), where k is the final number of clusters and n is the total number of data points to cluster.</li>
  <li><code>--estimatedDistanceCutoff (-e) &lt;estimatedDistanceCutoff&gt;</code>: The initial estimated distance cutoff between two points for forming new clusters. If no value is given, it’s estimated from the data set</li>
  <li><code>--maxNumIterations (-mi) &lt;maxNumIterations&gt;</code>: The maximum number of iterations to run for the BallKMeans algorithm used by the reducer. If no value is given, defaults to 10.</li>
  <li><code>--trimFraction (-tf) &lt;trimFraction&gt;</code>: The ‘ball’ aspect of ball k-means means that only the closest points to the centroid will actually be used for updating. The fraction of the points to be used is those points whose distance to the center is within trimFraction * distance to the closest other center. If no value is given, defaults to 0.9.</li>
  <li><code>--randomInit</code> (<code>-ri</code>) Whether to use k-means++ initialization or random initialization of the seed centroids. Essentially, k-means++ provides better clusters, but takes longer, whereas random initialization takes less time, but produces worse clusters, and tends to fail more often and needs multiple runs to compare to k-means++. If set, uses the random initialization.</li>
  <li><code>--ignoreWeights (-iw)</code>: Whether to correct the weights of the centroids after the clustering is done. The weights end up being wrong because of the trimFraction and possible train/test splits. In some cases, especially in a pipeline, having an accurate count of the weights is useful. If set, ignores the final weights.</li>
  <li><code>--testProbability (-testp) &lt;testProbability&gt;</code>: A double value  between 0 and 1  that represents  the percentage of  points to be used  for ‘testing’  different  clustering runs in  the final  BallKMeans step.  If no value is  given, defaults to  0.1</li>
  <li><code>--numBallKMeansRuns (-nbkm) &lt;numBallKMeansRuns&gt;</code>: Number of  BallKMeans runs to  use at the end to  try to cluster the  points. If no  value is given,  defaults to 4</li>
  <li><code>--distanceMeasure (-dm) &lt;distanceMeasure&gt;</code>: The classname of  the  DistanceMeasure.  Default is  SquaredEuclidean.</li>
  <li><code>--searcherClass (-sc) &lt;searcherClass&gt;</code>: The type of  searcher to be  used when  performing nearest  neighbor searches.  Defaults to  ProjectionSearch.</li>
  <li><code>--numProjections (-np) &lt;numProjections&gt;</code>: The number of  projections  considered in  estimating the  distances between  vectors. Only used  when the distance  measure requested is either ProjectionSearch or FastProjectionSearch. If no value is given, defaults to 3.</li>
  <li><code>--searchSize (-s) &lt;searchSize&gt;</code>: In more efficient  searches (non  BruteSearch), not all distances are calculated for determining the nearest neighbors. The number of elements whose distances from the query vector is actually computer is proportional to searchSize. If no value is given, defaults to 1.</li>
  <li><code>--reduceStreamingKMeans (-rskm)</code>: There might be too many intermediate clusters from the mapper to fit into memory, so the reducer can run  another pass of StreamingKMeans to collapse them down to a fewer clusters.</li>
  <li><code>--method (-xm)</code> method The execution  method to use:  sequential or  mapreduce. Default  is mapreduce.</li>
  <li><code>-- help (-h)</code>: Print out help</li>
  <li><code>--tempDir &lt;tempDir&gt;</code>: Intermediate output directory.</li>
  <li><code>--startPhase &lt;startPhase&gt;</code> First phase to run.</li>
  <li><code>--endPhase &lt;endPhase&gt;</code> Last phase to run.</li>
</ul>

<p>##References</p>

<ol>
  <li><a href="http://nips.cc/Conferences/2011/Program/event.php?ID=2989" title="M. Shindler, A. Wong, A. Meyerson: Fast and Accurate k-means For Large Datasets">M. Shindler, A. Wong, A. Meyerson: Fast and Accurate k-means For Large Datasets</a></li>
  <li><a href="http://www.math.uwaterloo.ca/~cswamy/papers/kmeansfnl.pdf" title="R. Ostrovsky, Y. Rabani, L. Schulman, Ch. Swamy: The Effectiveness of Lloyd-Type Methods for the k-means Problem">R. Ostrovsky, Y. Rabani, L. Schulman, Ch. Swamy: The Effectiveness of Lloyd-Type Methods for the k-means Problem</a></li>
</ol>


    </div>
</div>

  </div>


</div>

<div id="footer">
  <div class="container">
    <p>&copy; 2017 Apache Mahout
      with help from <a href="http://jekyllbootstrap.com" target="_blank" title="The Definitive Jekyll Blogging Framework">Jekyll Bootstrap</a>
      and <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>
    </p>
  </div>
</div>







<!-- Latest compiled and minified JavaScript, requires jQuery 1.x (2.x not supported in IE8) -->
<!-- Placed at the end of the document so the pages load faster -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="/assets/themes/mahout3/js/bootstrap.min.js"></script>
</body>
</html>

