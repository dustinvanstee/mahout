<h2 id="intro">Intro</h2>

<p>Mahout has a distributed implementation of QR decomposition for tall thin matrices<a href="[Mahout Scala and Mahout Spark Bindings for Linear Algebra Subroutines](http://mahout.apache.org/users/sparkbindings/ScalaSparkBindings.pdf)">1</a>.</p>

<h2 id="algorithm">Algorithm</h2>

<p>For the classic QR decomposition of the form <script type="math/tex">\mathbf{A}=\mathbf{QR},\mathbf{A}\in\mathbb{R}^{m\times n}</script> 
a distributed version is fairly easily achieved if <script type="math/tex">\mathbf{A}</script> is tall and thin such that 
<script type="math/tex">\mathbf{A}^{\top}\mathbf{A}</script> fits in memory, i.e. <em>m</em> is large but <em>n</em> &lt; ~5000 Under such circumstances, 
only <script type="math/tex">\mathbf{A}</script> and <script type="math/tex">\mathbf{Q}</script> are distributed matrices and <script type="math/tex">\mathbf{A^{\top}A}</script> and 
<script type="math/tex">\mathbf{R}</script> are in-core products. We just compute the in-core version of the Cholesky decomposition 
in the form of <script type="math/tex">\mathbf{LL}^{\top}= \mathbf{A}^{\top}\mathbf{A}</script>.  After that we take <script type="math/tex">\mathbf{R}= \mathbf{L}^{\top}</script>
 and <script type="math/tex">\mathbf{Q}=\mathbf{A}\left(\mathbf{L}^{\top}\right)^{-1}</script>.  The latter is easily achieved by multiplying each 
 vertical block of <script type="math/tex">\mathbf{A}</script> by <script type="math/tex">\left(\mathbf{L}^{\top}\right)^{-1}</script>.  (There is no actual matrix inversion 
 happening).</p>

<h2 id="implementation">Implementation</h2>

<p>Mahout <code class="highlighter-rouge">dqrThin(...)</code> is implemented in the mahout <code class="highlighter-rouge">math-scala</code> algebraic optimizer which translates Mahoutâ€™s R-like linear algebra operators into a physical plan for both Spark and H2O distributed engines.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def dqrThin[K: ClassTag](A: DrmLike[K], checkRankDeficiency: Boolean = true): (DrmLike[K], Matrix) = {        
    if (drmA.ncol &gt; 5000)
        log.warn("A is too fat. A'A must fit in memory and easily broadcasted.")
    implicit val ctx = drmA.context
    val AtA = (drmA.t %*% drmA).checkpoint()
    val inCoreAtA = AtA.collect
    val ch = chol(inCoreAtA)
    val inCoreR = (ch.getL cloned) t
    if (checkRankDeficiency &amp;&amp; !ch.isPositiveDefinite)
        throw new IllegalArgumentException("R is rank-deficient.")
    val bcastAtA = sc.broadcast(inCoreAtA)
    val Q = A.mapBlock() {
        case (keys, block) =&gt; keys -&gt; chol(bcastAtA).solveRight(block)
    }
    Q -&gt; inCoreR
}
</code></pre></div></div>

<h2 id="usage">Usage</h2>

<p>The scala <code class="highlighter-rouge">dqrThin(...)</code> method can easily be called in any Spark or H2O application built with the <code class="highlighter-rouge">math-scala</code> library and the corresponding <code class="highlighter-rouge">Spark</code> or <code class="highlighter-rouge">H2O</code> engine module as follows:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import org.apache.mahout.math._
import decompositions._
import drm._

val(drmQ, inCoreR) = dqrThin(drma)
</code></pre></div></div>

<h2 id="references">References</h2>

