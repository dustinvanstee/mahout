<p>You’ve probably already noticed Mahout has a lot of things going on at different levels, and it can be hard to know where
to start.  Let’s provide an overview to help you see how the pieces fit together. In general the stack is something like this:</p>

<ol>
  <li>Application Code</li>
  <li>Samsara Scala-DSL (Syntactic Sugar)</li>
  <li>Logical/Physical DAG</li>
  <li>Engine Bindings</li>
  <li>Code runs in Engine</li>
  <li>Native Solvers</li>
</ol>

<h2 id="application-code">Application Code</h2>

<p>You have an JAVA/Scala applicatoin (skip this if you’re working from an interactive shell or Apache Zeppelin)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def main(args: Array[String]) {

  println("Welcome to My Mahout App")

  if (args.isEmpty) {
</code></pre></div></div>

<p>This may seem like a trivial part to call out, but the point is important- Mahout runs <em>inline</em> with your regular application 
code. E.g. if this is an Apache Spark app, then you do all your Spark things, including ETL and data prep in the same 
application, and then invoke Mahout’s mathematically expressive Scala DSL when you’re ready to math on it.</p>

<h2 id="samsara-scala-dsl-syntactic-sugar">Samsara Scala-DSL (Syntactic Sugar)</h2>

<p>So when you get to a point in your code where you’re ready to math it up (in this example Spark) you can elegently express 
yourself mathematically.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>implicit val sdc: org.apache.mahout.sparkbindings.SparkDistributedContext = sc2sdc(sc)

val A = drmWrap(rddA)
val B = drmWrap(rddB) 

val C = A.t %*% A + A %*% B.t
</code></pre></div></div>

<p>We’ve defined a <code class="highlighter-rouge">MahoutDistributedContext</code> (which is a wrapper on the Spark Context), and two Disitributed Row Matrices (DRMs)
which are wrappers around RDDs (in Spark).</p>

<h2 id="logical--physical-dag">Logical / Physical DAG</h2>

<p>At this point there is a bit of optimization that happens.  For example, consider the</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>A.t %*% A
</code></pre></div></div>

<p>Which is</p>
<center>\(\mathbf{A^\intercal A}\)</center>

<p>Transposing a large matrix is a very expensive thing to do, and in this case we don’t actually need to do it. There is a
more efficient way to calculate <foo>\(\mathbf{A^\intercal A}\)</foo> that doesn’t require a physical transpose.</p>

<p>(Image showing this)</p>

<p>Mahout converts this code into something that looks like:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>OpAtA(A) + OpABt(A, B) //  illustrative pseudocode with real functions called
</code></pre></div></div>

<p>There’s a little more magic that happens at this level, but the punchline is <em>Mahout translates the pretty scala into a
a series of operators, which at the next level are turned implemented at the engine</em>.</p>

<h2 id="engine-bindings-and-engine-level-ops">Engine Bindings and Engine Level Ops</h2>

<p>When one creates new engine bindings, one is in essence defining</p>

<ol>
  <li>What the engine specific underlying structure for a DRM is (in Spark its an RDD).  The underlying structure also has 
rows of <code class="highlighter-rouge">MahoutVector</code>s, so in Spark <code class="highlighter-rouge">RDD[(index, MahoutVector)]</code>.  This will be important when we get to the native solvers.</li>
  <li>Implementing a set of BLAS (basic linear algebra) functions for working on the underlying structure- in Spark this means
implementing things like <code class="highlighter-rouge">AtA</code> on an RDD. See <a href="https://github.com/apache/mahout/tree/master/spark/src/main/scala/org/apache/mahout/sparkbindings">the sparkbindings on github</a></li>
</ol>

<p>Now your mathematically expresive Samsara Scala code has been translated into optimized engine specific functions.</p>

<h2 id="native-solvers">Native Solvers</h2>

<p>Recall how I said the rows of the DRMs are <code class="highlighter-rouge">org.apache.mahout.math.Vector</code>.  Here is where this becomes important. I’m going 
to explain this in the context of Spark, but the principals apply to all distributed backends.</p>

<p>If you are familiar with how mapping and reducing in Spark, then envision this RDD of <code class="highlighter-rouge">MahoutVector</code>s,  each partition, 
and indexed collection of vectors is a <em>block</em> of the distributed matrix, however this <em>block</em> is totally incore, and therefor
is treated like an in core matrix.</p>

<p>Now Mahout defines its own incore BLAS packs and refers to them as <em>Native Solvers</em>.  The default native solver is just plain
old JVM, which is painfully slow, but works just about anywhere.</p>

<p>When the data gets to the node and an operation on the matrix block is called.  In the same way Mahout converts abstract
operators on the DRM that are implemented on various distributed engines, it calls abstract operators on the incore matrix 
and vectors which are implemented on various native solvers.</p>

<p>The default “native solver” is the JVM, which isn’t native at all- and if no actual native solvers are present operations 
will fall back to this. However, IF a native solver is present (the jar was added to the notebook), then the magic will happen.</p>

<p>Imagine still we have our Spark executor- it has this block of a matrix sitting in its core. Now let’s suppose the <code class="highlighter-rouge">ViennaCl-OMP</code>
native solver is in use.  When Spark calls an operation on this incore matrix, the matrix dumps out of the JVM and the 
calculation is carried out on <em>all available CPUs</em>.</p>

<p>In a similar way, the <code class="highlighter-rouge">ViennaCL</code> native solver dumps the matrix out of the JVM and looks for a GPU to execute the operations on.</p>

<p>Once the operations are complete, the result is loaded back up into the JVM, and Spark (or whatever distributed engine) and 
shipped back to the driver.</p>

<p>The native solver operatoins are only defined on <code class="highlighter-rouge">org.apache.mahout.math.Vector</code> and <code class="highlighter-rouge">org.apache.mahout.math.Matrix</code>, which is 
why it is critical that the underlying structure composed row-wise of <code class="highlighter-rouge">Vector</code> or <code class="highlighter-rouge">Matrices</code>.</p>

